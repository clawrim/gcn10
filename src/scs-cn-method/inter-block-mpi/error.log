****************************************************************************
* hwloc 2.0.2rc1-git has encountered what looks like an error from the operating system.
*
* L1d (cpuset 0x00000103) intersects with L2 (cpuset 0x00000f00) without inclusion!
* Error occurred in topology.c line 1384
*
* The following FAQ entry in the hwloc documentation may help:
*   What should I do when hwloc reports "operating system" warnings?
* Otherwise please report this error message to the hwloc user's mailing list,
* along with the files generated by the hwloc-gather-topology script.
****************************************************************************
[abd:16394] *** Process received signal ***
[abd:16394] Signal: Segmentation fault (11)
[abd:16394] Signal code: Address not mapped (1)
[abd:16394] Failing at address: (nil)
[abd:16394] [ 0] /lib64/libc.so.6(+0x430d0) [0x7faf2fa430d0]
[abd:16394] [ 1] /usr/lib64/libopen-pal.so.40(opal_hwloc201_hwloc_bitmap_copy+0x10) [0x7faf2ff15fd0]
[abd:16394] [ 2] /usr/lib64/libopen-pal.so.40(+0x9d3b2) [0x7faf2ff3a3b2]
[abd:16394] [ 3] /usr/lib64/libopen-pal.so.40(+0x9d4b1) [0x7faf2ff3a4b1]
[abd:16394] [ 4] /usr/lib64/libopen-pal.so.40(+0x9d4b1) [0x7faf2ff3a4b1]
[abd:16394] [ 5] /usr/lib64/libopen-pal.so.40(+0x9d4b1) [0x7faf2ff3a4b1]
[abd:16394] [ 6] /usr/lib64/libopen-pal.so.40(opal_hwloc201_hwloc_topology_load+0x1dc) [0x7faf2ff3f9dc]
[abd:16394] [ 7] /usr/lib64/libopen-pal.so.40(opal_hwloc_base_get_topology+0xfe8) [0x7faf2ff13b18]
[abd:16394] [ 8] /usr/lib64/openmpi/mca_ess_hnp.so(+0x5936) [0x7faf2fd0e936]
[abd:16394] [ 9] /usr/lib64/libopen-rte.so.40(orte_init+0x2ae) [0x7faf300186ce]
[abd:16394] [10] /usr/lib64/libopen-rte.so.40(orte_daemon+0x6a0) [0x7faf2ffc0750]
[abd:16394] [11] /lib64/libc.so.6(+0x29d57) [0x7faf2fa29d57]
[abd:16394] [12] /lib64/libc.so.6(__libc_start_main+0x85) [0x7faf2fa29e15]
[abd:16394] [13] orted() [0x401111]
[abd:16394] *** End of error message ***
[abd:16393] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file ess_singleton_module.c at line 716
[abd:16393] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file ess_singleton_module.c at line 172
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  orte_ess_init failed
  --> Returned value Unable to start a daemon on the local node (-127) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Unable to start a daemon on the local node" (-127) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[abd:16393] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
